{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "atremis_model_attempt4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUEUwQXbhuB_"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "import time \n",
        "import random\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFSq_o2FKTx3"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import spacy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from typing import List\n",
        "\n",
        "from torchtext.legacy.data import Field, BucketIterator"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocw98Xh3h2pS"
      },
      "source": [
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tccdE2EXNNsv"
      },
      "source": [
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 15\n",
        "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83JE9GMlZ2Gk"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Lumiere/Dataset/artemis_data.csv\"\n",
        "artemis_data = pd.read_csv(path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3cRHCELp_CH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "8f99cccd-57ec-4c29-ff34-4480257aa5fe"
      },
      "source": [
        "artemis_data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXT</th>\n",
              "      <th>LABEL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>She seems very happy in the picture, and you w...</td>\n",
              "      <td>She seems very happy in the picture, and you w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This woman has really knotty hands which makes...</td>\n",
              "      <td>This woman has really knotty hands which makes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>When looking at this woman, I am filled with c...</td>\n",
              "      <td>When looking at this woman, I am filled with c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A woman looking at ease, peaceful, and satisfi...</td>\n",
              "      <td>A woman looking at ease, peaceful, and satisfi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>She looks like a lady from that past that migh...</td>\n",
              "      <td>She looks like a lady from that past that migh...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                TEXT                                              LABEL\n",
              "0  She seems very happy in the picture, and you w...  She seems very happy in the picture, and you w...\n",
              "1  This woman has really knotty hands which makes...  This woman has really knotty hands which makes...\n",
              "2  When looking at this woman, I am filled with c...  When looking at this woman, I am filled with c...\n",
              "3  A woman looking at ease, peaceful, and satisfi...  A woman looking at ease, peaceful, and satisfi...\n",
              "4  She looks like a lady from that past that migh...  She looks like a lady from that past that migh..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SbiEBnNi4Pu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b53739-c2b9-48dd-deb5-02e480e3f6f5"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.2.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vh5s397i6q3"
      },
      "source": [
        "TEXT = torchtext.legacy.data.Field(sequential=True, use_vocab=True,\n",
        "    tokenize='spacy', # default splits on whitespace\n",
        "    tokenizer_language='en', init_token=\"<sos>\", eos_token=\"<eos>\" #not used lower=True\n",
        ")\n",
        "\n",
        "LABEL = torchtext.legacy.data.Field(sequential=True, use_vocab=True,\n",
        "    tokenize='spacy', # default splits on whitespace\n",
        "    tokenizer_language='en', init_token=\"<sos>\", eos_token=\"<eos>\"\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA7CJBiNjBAx"
      },
      "source": [
        "fields = [(\"TEXT\", TEXT), (\"LABEL\", LABEL)]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO35iKSajE7o"
      },
      "source": [
        "dataset = torchtext.legacy.data.TabularDataset(\n",
        "    path=path, format='csv',\n",
        "    skip_header=True, fields=fields)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DpHMpUBjRq9",
        "outputId": "0ab80c82-2485-4d8a-81cb-8a41d91b1790"
      },
      "source": [
        "train_data, valid_data, test_data = dataset.split(\n",
        "    split_ratio=[0.8, 0.1, 0.1], # 80/10/10 split\n",
        "    random_state=random.seed(random_seed))\n",
        "\n",
        "print(f'Num Train: {len(train_data.examples)}')\n",
        "print(f'Num Validation: {len(valid_data.examples)}')\n",
        "print(f'Num Test: {len(test_data.examples)}')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num Train: 363747\n",
            "Num Validation: 45469\n",
            "Num Test: 45468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sItm7KWjgyQ",
        "outputId": "b56ac3d0-ab1d-4ed0-b53c-13f81bc84bcd"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'TEXT': ['One', 'brave', 'knight', 'is', 'taking', 'on', 'a', 'team', 'of', 'opposing', 'knights', 'in', 'a', 'forest', 'field', '.'], 'LABEL': ['One', 'brave', 'knight', 'is', 'taking', 'on', 'a', 'team', 'of', 'opposing', 'knights', 'in', 'a', 'forest', 'field', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLdhZMOsjk2g",
        "outputId": "f9a58bb0-5634-4d39-eab4-48cf9650100e"
      },
      "source": [
        "TEXT.build_vocab(train_data, min_freq=2, vectors=\"glove.6B.100d\") #vectors=\"glove.6B.100d\" not working <urlopen error [Errno 111] Connection refused>\n",
        "\n",
        "print(f'Vocabulary size: {len(TEXT.vocab)}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:42, 5.32MB/s]                           \n",
            "100%|█████████▉| 399312/400000 [00:18<00:00, 20432.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 29656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJrOQ-3Tkw7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6920069c-b4aa-4e14-b921-8cdfd79b2f4d"
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 364835), ('.', 321266), ('and', 205470), ('a', 177577), ('of', 168372), ('The', 162589), ('is', 144451), (',', 114619), ('to', 106939), ('in', 100437), ('like', 95528), ('I', 77201), ('looks', 69386), ('it', 61362), ('me', 61302), ('are', 56835), ('this', 55337), ('on', 50688), ('with', 46207), (\"'s\", 44195)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V0uVLVPk2zq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84df771-e302-4218-d1b4-ce4bad379848"
      },
      "source": [
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', '<sos>', '<eos>', 'the', '.', 'and', 'a', 'of', 'The']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaAYuSiKk8n5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3b10af-8c4e-4247-99ac-b9e19a570c8c"
      },
      "source": [
        "print(TEXT.vocab.stoi['art'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnkY3xfTsB40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921a459e-1299-44dd-d019-19dab32d76f5"
      },
      "source": [
        "LABEL.build_vocab(train_data, min_freq=2, vectors=\"glove.6B.100d\") # same here\n",
        "print(f'Number of classes: {len(LABEL.vocab)}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of classes: 29656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd84KNldsbne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaffdfc3-8256-406e-e6a3-ce120da14132"
      },
      "source": [
        "print(LABEL.vocab.stoi[\"art\"])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfbPHXyQlBIn"
      },
      "source": [
        "train_loader, valid_loader, test_loader = torchtext.legacy.data.BucketIterator.splits(\n",
        "        (train_data, valid_data, test_data),\n",
        "         batch_size=BATCH_SIZE,\n",
        "         sort_within_batch=False,\n",
        "         sort_key=lambda x: len(x.TEXT),\n",
        "         device=DEVICE\n",
        "    )"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HfgNJU-lJsN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2afa6867-c56a-46bd-ade5-982043cd06ca"
      },
      "source": [
        "print(' Train')\n",
        "for batch in train_loader:\n",
        "    print(f'Text size: {batch.TEXT.size()}')\n",
        "    print(f'Target size: {batch.LABEL.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\n', 'Valid:')\n",
        "for batch in valid_loader:\n",
        "    print(f'Text size: {batch.TEXT.size()}')\n",
        "    print(f'Target vector size: {batch.LABEL.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\n', 'Test:')\n",
        "for batch in test_loader:\n",
        "    print(f'Text size: {batch.TEXT.size()}')\n",
        "    print(f'Target size: {batch.LABEL.size()}') \n",
        "    break\n",
        "\n",
        "#[SENTENCE LENGTH, BATCH SIZE] #sentences of only size 6?"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Train\n",
            "Text size: torch.Size([65, 128])\n",
            "Target size: torch.Size([65, 128])\n",
            "\n",
            " Valid:\n",
            "Text size: torch.Size([6, 128])\n",
            "Target vector size: torch.Size([6, 128])\n",
            "\n",
            " Test:\n",
            "Text size: torch.Size([6, 128])\n",
            "Target size: torch.Size([6, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fit6vi0nZae5",
        "outputId": "ee207dca-3b40-4245-9f6f-f40d4cd3ef8a"
      },
      "source": [
        "try_one = next(iter(test_loader))\n",
        "try_one"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.legacy.data.batch.Batch of size 128]\n",
              "\t[.TEXT]:[torch.LongTensor of size 6x128]\n",
              "\t[.LABEL]:[torch.LongTensor of size 6x128]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8qe49eMZcI8",
        "outputId": "7678c7f5-d3c7-4d4d-d44a-e21e5293a472"
      },
      "source": [
        "try_one.TEXT"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2],\n",
              "        [    0,     0,     0,     0,  8414,     0,  3355,  3158,     0,    30,\n",
              "           852, 24468,   852, 19743,     0,   852,   478,  4144,    15,     9,\n",
              "           321,  1254,  3651,    62,  5320,    62, 10508,  4841,  2337,   852,\n",
              "             9,     9,     7,     0,    62,    52,    17,  1893,     9,   274,\n",
              "            51,   367,    48,     0,     0, 17734,  1721,     4,     7,    51,\n",
              "          2860,    49,    62,   518,   564,    51,  5475,   852,    51,   796,\n",
              "           852,     9,    20,   523,  1292,    48,  1228,     4, 14660,   216,\n",
              "           852,  2217,    97,    53,     4, 15929,     9,   116,     4, 20139,\n",
              "            62,     4, 20404,   567,   237,    97,   216,     7,   236,     9,\n",
              "            49,    62,   441,     4,    36,    46,    24,    35,    42,     9,\n",
              "            93,  1314,     4,  4749,    15,  1742,   402,     4,   523,     4,\n",
              "            86,     0,    62,  4841,     4,     4,  2741,   665,     9,    62,\n",
              "             4, 11453,   523,    62,     4,   144,     4,   498],\n",
              "        [    3,     3,     3,     3,     3,     3,     3,     3,     3,  1955,\n",
              "          4509,    32,   869,   397,  3102,  4509,   120,  8968,  3520,   871,\n",
              "            16,     8,   398,  2801,   679,    69,  1140,    12,   560,    13,\n",
              "           213,   994,  7284,     8,   242,    49,    16,     8,  2038,    68,\n",
              "            16,    19,    26,     6,   159, 26258,  3340,   344,    69,    26,\n",
              "           239,    30,   577,   176,    26,    49,   418,   452,    16,  7871,\n",
              "           689,  1370,    10,   230,     6,    26,    10,  3016,   207,    14,\n",
              "            13,     6,    70,     6,   191,    32,     0,  1911,  2263,    72,\n",
              "            69,  8479,   936,   328,    61,    57,    14,    30,   570,    44,\n",
              "           512,   202,  1905,   131,    10,   283,   101,   110,    53,  3409,\n",
              "           215,   283,   381,  5915,    28, 12925,    10,   247,     7,  1945,\n",
              "            14,     0,   174,   543,  2100,   207, 10726,     6,   881, 12095,\n",
              "           142,  2203,     7,   188,  1799,    46,   211,    46],\n",
              "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,   904,     0, 11028,\n",
              "           241,    27,  1702,    50,     5,   168,     5,   270,   429,  2149,\n",
              "             5,   658,    50,   484,    33,    69,  4419,   659,     5,   113,\n",
              "          1943,   349,   455, 21732,   631,  1718,   411,  1995,    33,  2573,\n",
              "           419,   445,    33,     5,   846,  2324,    25,   150,  1862, 12796,\n",
              "           162,     5,   586,     0,  7076,   387,   743,   708,     5, 19410,\n",
              "          1054, 11771,    25,   344,  1355,     5,    50,   169,  1049,  2062,\n",
              "            32,    50,   716,    32,   175,   256,  3699,    97,    30,    26,\n",
              "             6,   142, 16312,    16,   165,     8,    26,    26,     6,    26,\n",
              "            26,   169,    16,   126,     4,    33,  9297,    16,    95,    19,\n",
              "             4,     0,   249,    65,    19,    19,    11,   287,    16,  5716,\n",
              "            16,   673,   143,   454,    10,  6110,    10,  1108],\n",
              "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,    27,   789,   170,  1888,  2364,\n",
              "            53,     5,   129,  1621,   283,  4445,   141,    69,   344,  1262,\n",
              "          2926,     5,   686,     5,  2248,     5,   150,   562,    50,   191,\n",
              "          2672,   470,   210,  5528,  5438,     0,  1166,     5,   561,  6868,\n",
              "           327,     5,  2149,   100,   201,    60,   188,   506],\n",
              "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue1EQbe-xJOf"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "OUTPUT_DIM = len(LABEL.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "LAT_DIM = 100\n",
        "N_LAYERS = 1 # N layers = 2 is complicated\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5 "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uLBI5EYRS5X"
      },
      "source": [
        "#ENCODER\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim: int, emb_dim: int, hid_dim: int, latent_dim:int, n_layers: int, dropout: float):\n",
        "        super(Encoder,self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) #[25805-> 256] [len(TEXT.vocab)->embed dim]\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout) #[256 -> 512] [embed dim -> hidden dim]\n",
        "        \n",
        "\n",
        "        self.z_mean = torch.nn.Linear(hid_dim, latent_dim)  #[512 -> 100] [hidden dim -> latent dim]\n",
        "        self.z_logvar = torch.nn.Linear(hid_dim, latent_dim) #[512 -> 100] [hidden dim -> latent dim]\n",
        "\n",
        "    def reparameterise(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        eps = std.data.new(std.size()).normal_() # sample from normal distribution\n",
        "        return eps.mul(std).add_(mu)\n",
        "    \n",
        "    def forward(self, x: torch.LongTensor):\n",
        "        embedded = self.embedding(x) # [sentence len, batch size, embed dim]\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        mu = self.z_mean(hidden)\n",
        "        logvar = self.z_logvar(hidden)\n",
        "        # epsilon = torch.randn([batch_size, self.hidden_dim]) # may be problems here: maybe [1, batch_size, hidden_dim]\n",
        "        z = self.reparameterise(mu, logvar) \n",
        "\n",
        "        return z, mu, logvar #we do not need to return mu and logvar\n",
        "        "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuH1HQs8NKHo",
        "outputId": "dffc68ed-0d62-4191-f431-5848c9819551"
      },
      "source": [
        "try_one.TEXT.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5jjXIbRIrkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "422b54de-60ba-4527-a24b-51f0a0609e8e"
      },
      "source": [
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, LAT_DIM, N_LAYERS, ENC_DROPOUT).to(DEVICE)\n",
        "z_try, _, _= encoder(try_one.TEXT)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLV33KbmJMEj",
        "outputId": "7a2bb4ff-3c51-4f8b-d935-b092ffbc9f59"
      },
      "source": [
        "z_try.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2muTrxVLl7-",
        "outputId": "1169324d-492f-437c-eedd-f90c7e21fb95"
      },
      "source": [
        "encoder"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoder(\n",
              "  (embedding): Embedding(29656, 256)\n",
              "  (rnn): LSTM(256, 512, dropout=0.5)\n",
              "  (z_mean): Linear(in_features=512, out_features=100, bias=True)\n",
              "  (z_logvar): Linear(in_features=512, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HywPRoIlVf-6"
      },
      "source": [
        "#DECODER\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, input_dim:int, emb_dim: int, hid_dim: int, latent_dim:int, n_layers: int, dropout: float):\n",
        "      super(Decoder, self).__init__()\n",
        "\n",
        "      self.input_dim = input_dim\n",
        "      self.emb_dim = emb_dim\n",
        "      self.hid_dim = hid_dim\n",
        "      self.latent_dim = latent_dim\n",
        "      self.n_layers = n_layers\n",
        "      self.dropout = dropout\n",
        "\n",
        "      # is it nn.linear(2*latent dim, hidden dim) -> check\n",
        "      self.linear1 = torch.nn.Linear(latent_dim, emb_dim) #[100-> 256] [2*latent dim -> embed dim]\n",
        "      # embed?\n",
        "      self.rnn_decoder = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)  #[256 -> 512] [embed dim -> hidden dim] # not sure about this here\n",
        "      self.linear2 = torch.nn.Linear(hid_dim, input_dim) #[512 -> 25805] [hidden dim -> len(TEXT.vocab)]\n",
        "\n",
        "  def forward(self, z):\n",
        "      output_lat = self.linear1(z.unsqueeze(0)) #requires 3 dimensions [1, batch size, emb dim] sentence len = 1 (decode one at a time)\n",
        "      output_lstm, (hidden, cell_state) = self.rnn_decoder(output_lat.squeeze(0))\n",
        "      output = self.linear2(output_lstm.squeeze(0)) # squeezing out the [1,]\n",
        "      return output # we do not need to return hidden, cell state; squeeze is to remove the nlayers dimension\n",
        "    "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZmTX3v-K4ip",
        "outputId": "a0059a54-db9e-4b40-c343-a0184ab269f0"
      },
      "source": [
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM,LAT_DIM, N_LAYERS, DEC_DROPOUT).to(DEVICE)\n",
        "decoder"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (linear1): Linear(in_features=100, out_features=256, bias=True)\n",
              "  (rnn_decoder): LSTM(256, 512, dropout=0.5)\n",
              "  (linear2): Linear(in_features=512, out_features=29656, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIR-mIkHL7cf"
      },
      "source": [
        "out_try = decoder(z_try)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atRHlIRyM9Je",
        "outputId": "9e830d07-01ac-4a94-fac3-160288825670"
      },
      "source": [
        "out_try.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 29656])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRjO4N4NOebL"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder:Encoder, decoder:Decoder, device: torch.device):\n",
        "\n",
        "    super(VAE,self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "\n",
        "    assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "              'Hidden dimensions of encoder and decoder must be equal!'\n",
        "    assert encoder.n_layers == decoder.n_layers, \\\n",
        "              'Encoder and decoder must have equal number of layers!'\n",
        "  \n",
        "  def forward(self, x:torch.LongTensor):\n",
        "    max_len, batch_size = x.shape \n",
        "    v_size = self.decoder.input_dim # size of vocab\n",
        "    outputs = torch.zeros(max_len, batch_size, v_size).to(self.device) #store outputs\n",
        "\n",
        "    z,mu,logvar = self.encoder(x)\n",
        "\n",
        "    # trg = x[0] -> not the case as we have to pass in z to the decoder.\n",
        "\n",
        "    # decoding one at a time\n",
        "    for i in range(1, max_len):\n",
        "      prediction = self.decoder(trg) #not sure what to do here\n",
        "\n",
        "    output = self.decoder(z)\n",
        "\n",
        "    return output, mu, logvar\n",
        "  "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5FKQXAXTFHo",
        "outputId": "1044c5a1-c045-4594-f3de-5f7e9d2d5b50"
      },
      "source": [
        "vae = VAE(encoder, decoder, DEVICE).to(DEVICE)\n",
        "vae"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VAE(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(29656, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (z_mean): Linear(in_features=512, out_features=100, bias=True)\n",
              "    (z_logvar): Linear(in_features=512, out_features=100, bias=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (linear1): Linear(in_features=100, out_features=256, bias=True)\n",
              "    (rnn_decoder): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (linear2): Linear(in_features=512, out_features=29656, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPrmyrJdTMEu"
      },
      "source": [
        "outputs,_,_ = vae(try_one.TEXT)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDYx011HTSky",
        "outputId": "0b930025-d847-40c4-e7ad-4bf3adc9d4f4"
      },
      "source": [
        "outputs.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 128, 29656])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVhknP9eN6CY"
      },
      "source": [
        "def rec_kl_loss(yhat, y, mu, logvar):\n",
        "  BCE = nn.functional.binary_cross_entropy(yhat, y, reduction=\"sum\")\n",
        "  KLD = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2))\n",
        "  loss = BCE + KLD\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUXI7joRTrvQ"
      },
      "source": [
        "lr = 1e-3\n",
        "optimizer = optim.Adam(vae.parameters(), lr=lr)\n",
        "\n",
        "PAD_IDX = TEXT.vocab.stoi[\"<pad>\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyIdJ01BVSTh"
      },
      "source": [
        "def train(vae, iterator, optimizer, criterion):\n",
        "    vae.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = vae(batch.TEXT)\n",
        "        outputs_flatten = outputs[0][1:].view(-1, outputs[0].shape[-1]) #These shapes dont work for my model\n",
        "        label_flatten = batch.LABEL.view(-1)\n",
        "        loss = criterion(outputs_flatten, label_flatten, outputs[1], outputs[2])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1o9iok-WN9a"
      },
      "source": [
        "def evaluate(vae, iterator, criterion):\n",
        "    vae.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "           \n",
        "            outputs = vae(batch.TEXT) \n",
        "            outputs_flatten = outputs[0][1:].view(-1, outputs[0].shape[-1])\n",
        "            label_flatten = batch.LABEL.view(-1, outputs[0].shape[-1])\n",
        "            loss = criterion(outputs_flatten, label_flatten, outputs[1], outputs[2])\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR3-Z378Wfpg"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5uZUON7Whyy"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):    \n",
        "    start_time = time.time()\n",
        "    train_loss = train(vae, train_loader, optimizer, rec_kl_loss)\n",
        "    valid_loss = evaluate(vae, valid_loader, rec_kl_loss)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(vae.state_dict(), 'model1.pt')\n",
        "\n",
        "    # it's easier to see a change in perplexity between epoch as it's an exponential\n",
        "    # of the loss, hence the scale of the measure is much bigger\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}